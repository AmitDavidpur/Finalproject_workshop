# -*- coding: utf-8 -*-
"""Python_workshop_Final-Amit&Meshi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PO8EoB845lW5T8GVMbPRtnCJl88mFC7D

**Amit Davidpur** - 207496878

**Meshi Ben Oz** - 207287566
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC

"""# Preprocessing"""

# Upload csv of data from Kaggle
df = pd.read_csv('healthcare-dataset-stroke-data.csv', index_col='id')
df

# Display the DataFrame information
df.info()

# Convert the columns type to object to preform separate analysis on numeric vs. categorical data
cat_columns = ['hypertension', 'heart_disease','smoking_status','stroke']
df[cat_columns] = df[cat_columns].astype('object')

# Descriptive statistics
df.describe()

# Check for duplications in the data
df.duplicated().sum()

# Check for missing values in the data
df.isna().sum()

# Visualization of categorical columns
object_columns = df.select_dtypes(include='object').columns # Select all columns with 'object' data type (categorical data)

# Iterate through each categorical column
for column in object_columns:
  # Value counts for each column
  value_counts = df[column].value_counts()
  print(value_counts)

  # Plot the frequency as a bar chart
  plt.figure(figsize=(6, 4))
  value_counts.plot(kind='bar', color='skyblue', edgecolor='black')
  plt.title(f"Value Counts for {column}")
  plt.xlabel(column)
  plt.ylabel("Count")
  plt.tight_layout()
  plt.show()

"""* From the graphs, we can observe that most of the patients are healthy and have not had a stroke, which indicates an imbalanced dataset. Additionally, the majority of patients are women, and most have been married at some point in their lives or are currently married."""

# Visualization of numerical columns
float_columns = df.select_dtypes(include='float').columns # Select all columns with 'float' data type (numerical data)

# Plot histograms to visualize the distribution
for column in float_columns:
    plt.figure(figsize=(6, 4))
    df[column].plot(kind='hist', bins=10, color='lightgreen', edgecolor='black')
    plt.title(f"Histogram for {column}")
    plt.xlabel(column)
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

"""* Based on the histograms, we can see that the age distribution is close to a normal distribution, with a peak around the age of 55. Most patients have an average glucose level between 70 and 100, which is considered a normal range according to medical literature. Additionally, with regard to BMI, most patients fall within the range of 20-30.

* The data has missing values in the BMI column. After some consideration, we decided to group all patients based on gender and average glucose level. Based on these groups, we filled the missing BMI values using the median value of each group.
"""

# Grouping by gender and glucose level, calculating the median BMI
df['Glucose_bin'] = pd.cut(df['avg_glucose_level'], bins=4, labels=False)
median_bmi = df.groupby(['gender', 'Glucose_bin'])['bmi'].median().reset_index()
median_bmi = median_bmi.rename(columns={'bmi': 'Median_BMI'})

# Filling missing BMI values with the median BMI
df = pd.merge(df, median_bmi, on=['gender', 'Glucose_bin'], how='left')
df['bmi'] = df['bmi'].fillna(df['Median_BMI'])

df

# After filling the missing values we remove the columns we created for grouping
df = df.drop(columns=['Glucose_bin', 'Median_BMI'])
df

# Check again for missing values
df.isna().sum()

# Define a function to encode categorical columns into numerical values based on a given dictionary
def encode_columns(df, encoding_dict):
  df_copy = df.copy()
  for column, mapping in encoding_dict.items():
    if column in df_copy.columns:
      # Map the values in the column using the provided encoding
      df_copy[column] = df_copy[column].map(mapping).astype(int)
  return df_copy

# Define a dictionary with the encoding mappings for categorical columns
encoding_dict = {
  'gender': {'Male': 0, 'Female': 1, 'Other': 2},
  'ever_married': {'Yes': 1, 'No': 0},
  'work_type': {'Private': 0, 'Self-employed': 1, 'children': 2, 'Govt_job': 3, 'Never_worked': 4},
  'Residence_type': {'Urban': 0, 'Rural': 1},
  'smoking_status': {'never smoked': 0, 'Unknown': 1, 'formerly smoked': 2, 'smokes': 3}
}

# Apply the encoding function on the DataFrame
df_new = encode_columns(df, encoding_dict)
df_new

"""# Features analysis and balance

### Correlation between the different features
As described in the article, we utilized Pearson's correlation coefficient to determine the relationships between various patient features. A correlation heatmap was created, where red indicates a positive correlation and blue indicates a negative correlation. The intensity of the color reflects the strength of the correlation, with deeper shades representing stronger relationships between the features.
"""

# Calculate the Pearson correlation matrix
corr_mat = df_new.corr(method='pearson')

# Plot the heatmap for the correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation heatmap')
plt.show()

"""* Based on the heatmap, there is a strong positive correlation between age and marital status (ever_married), with a correlation value of 0.68.
However, for the other features, there is little to no correlation.

### PCA and Scree plot

Following the study's approach, we applied PCA to transform the dataset into uncorrelated principal components, capturing maximum variance. Features were standardized by scaling to have a mean of zero and a standard deviation of one. A scree plot was used to identify components explaining most of the variance.
"""

# split and normalize the data
def split_data(df, outcome):
  y = df[outcome].astype(int)
  X = df.drop(columns=[outcome])

  # Standardize features
  scaler = StandardScaler()
  X_scaled = scaler.fit_transform(X)

  # Split the dataset
  X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

  return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = split_data(df_new, 'stroke')

"""**explained_variance_ratio_**ndarray of shape (n_components,)
Percentage of variance explained by each of the selected components.
 If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.

 A **scree plot** is a graph of eigenvalues against the corresponding PC number.
"""

# PCA and scree plot
def pca_analysis(df, X_train, status):
  # Perform PCA
  pca = PCA()
  X_pca = pca.fit_transform(X_train)

  # Plot the Scree plot
  explained_variance = pca.explained_variance_ratio_

  plt.figure(figsize=(8, 6))
  plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
  plt.title(f'Scree Plot - {status}')
  plt.xlabel('Principal Components')
  plt.ylabel('Explained Variance Ratio')
  plt.xticks(range(1, len(explained_variance) + 1))
  plt.grid(True)
  plt.show()

  # Displaying the explained variance percentages
  explained_variance_percent = explained_variance * 100
  print("Explained variance by each principal component:")
  for i, var in enumerate(explained_variance_percent, 1):
      print(f"PC{i}: {var:.2f}%")

  return X_pca, pca

# Call the func to analyse PCA
X_pca, pca = pca_analysis(df_new, X_train, 'Before balance')

"""In the paper, 10 principal components were derived, with 8 components explaining 88.2% of the variance.

In our analysis, as shown above, the variance explained by the top 8 principal components was 89.81%

---
We check the contributions of the different features in the first and second principal components.
"""

# Function to evaluate the contribution of the features to the PCA dimensions
def pca_contribution(pca, features):
  # Get the loadings for PC1 and PC2
  loadings = pca.components_.T[:, :2]

  # Create a DataFrame to display the loadings
  loadings_df = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=features)

  # Get absolute values of the loadings
  abs_loadings_df = round(loadings_df.abs(), 2)

  # Sort the features by their absolute contribution for PC1
  sorted_pc1 = abs_loadings_df['PC1'].sort_values(ascending=False)

  # Sort the features by their absolute contribution for PC2
  sorted_pc2 = abs_loadings_df['PC2'].sort_values(ascending=False)

  # Display the sorted results
  print("Sorted Contributions of Features to PC1:")
  print(sorted_pc1)
  print("\nSorted Contributions of Features to PC2:")
  print(sorted_pc2)

features = ['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',
            'work_type', 'residence_type', 'avg_glucose_level', 'bmi', 'smoking_status']

pca_contribution(pca, features)

"""In the paper, age and ever_married are among the most significant contributors to PC1, similar to our findings. For PC2, both analyses highlight gender and work_type as key contributors, with noticeable similarities in their ranking.

---
#### **Balance the data**

The original dataset
is unbalanced because there are more samples possessing negative
stroke labels, as compared to positive label stroke samples. We make
it balanced by considering all the positive stroke samples, and then
randomly picking equal number of negative stroke samples from the
rest. This will make a balanced dataset with equal number of positive
and negative stroke samples.
"""

# Count the number of samples before balancing
before_balancing = df_new['stroke'].value_counts()
print(f'Before balancing:\n{before_balancing}')

# Separate the dataset into stroke ans non-stroke cases
stroke_cases = df_new[df_new['stroke'] == 1]
non_stroke_cases = df_new[df_new['stroke'] == 0]

# Randomly select an equal number of non stroke cases as there are stroke cases
random_non_stroke = non_stroke_cases.sample(n=len(stroke_cases), random_state=42)

# Combine the stroke and selected non-stroke cases
balanced_df = pd.concat([stroke_cases, random_non_stroke])

# Shuffle the balanced dataset
balanced_df = shuffle(balanced_df, random_state=42).reset_index(drop=True)

# Count the number of samples after balancing
after_balancing = balanced_df['stroke'].value_counts()
print(f'\nAfter balancing:\n{after_balancing}')

balanced_df

# Split balanced data to train and test
X_train_bal, X_test_bal, y_train_bal, y_test_bal = split_data(balanced_df, 'stroke')

"""After balancing the dataset, we performed PCA and included a scree plot to examine whether the variance in the data and the features contributing most to it remained the same."""

# Apply the function to analyse PCA with balanced data
X_pca_bal, pca_bal = pca_analysis(balanced_df, X_train_bal, 'After balance')

pca_contribution(pca_bal, features)

"""We observed that, consistent with the findings reported in the paper, the contributions of the features remained largely unaffected after balancing the dataset. Age and ever_married had the highest contributions to PC1, while gender and heart disease contributed the most to PC2.

# Models
"""

# A function for evaluating model preformance
def calculate_metrics(y_true, y_pred):
    # Calculate confusion matrix
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

    # Calculate metrics
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f_score = f1_score(y_true, y_pred)
    accuracy = accuracy_score(y_true, y_pred)
    miss_rate = fn / (fn + tp)
    fallout_rate = fp / (fp + tn)

    return precision, recall, f_score, accuracy, miss_rate, fallout_rate

"""Our goal was to identify the best hyperparameters for each model. To achieve this, as learned in class, we created a function that performs grid search to find the hyperparameters that yield the optimal model."""

def grid_search_func(X_train, y_train, X_test, y_test, models, param_grids):

    # Initialize an empty list to store results
    results_list = []

    # Loop through each model and perform grid search
    for model_name in models:
        grid_search = GridSearchCV(models[model_name], param_grids[model_name], cv=5, n_jobs=-1)
        grid_search.fit(X_train, y_train)

        # Get the best model and make predictions
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test)

        # Get best parameters for the model
        best_params = grid_search.best_params_
        print(f"{model_name}'s best parameters:{best_params}")

        # Calculate the metrics for the best model
        precision, recall, f_score, accuracy, miss_rate, fallout_rate = calculate_metrics(y_test, y_pred)

        results = {
            'Model': model_name,
            'Precision': round(precision, 2),
            'Recall': round(recall, 2),
            'F-Score': round(f_score, 2),
            'Accuracy': round(accuracy, 2),
            'Miss rate': round(miss_rate, 2),
            'Fall-out rate': round(fallout_rate, 2),
        }

        # Append the results to the list
        results_list.append(results)

    # Convert results to a DataFrame
    results_df = pd.DataFrame(results_list)

    return results_df

"""First, we tested three models as done in the paper: Decision Tree, Random Forest, and Neural Network. For each model, we selected hyperparameters to search for the best ones."""

# Define the models
models = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Neural Network': MLPClassifier(max_iter=1000, random_state=42)
}

# Define parameter grids for each model
param_grids = {
    'Decision Tree': {
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'Random Forest': {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'Neural Network': {
        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
        'activation': ['relu', 'tanh'],
        'solver': ['adam', 'sgd']
    }
}

"""As outlined in the paper, we initially evaluated the models on the balanced dataset using all features, and then tested them on the balanced dataset reduced to 2 and 8 principal components using PCA. This approach allowed us to assess the models' performance across different levels of feature reduction.

For each model, we used the grid search function to find the best hyperparameters and displayed them. Additionally, we presented the accuracy and other evaluation metrics for each model with the optimal hyperparameters.
"""

# Print the results
results_df = grid_search_func(X_train_bal, y_train_bal, X_test_bal, y_test_bal, models, param_grids)
results_df

# Select the first 2 principal components
X_train_pca_2 = X_pca_bal[:, :2]
X_test_pca_2 = pca_bal.transform(X_test_bal)[:, :2]

# Select the first 8 principal components
X_train_pca_8 = X_pca_bal[:, :8]
X_test_pca_8 = pca_bal.transform(X_test_bal)[:, :8]

print("Training models with the first 2 principal components:")
results_pca_2 = grid_search_func(X_train_pca_2, y_train_bal, X_test_pca_2, y_test_bal, models, param_grids)
results_pca_2

print("Training models with the first 8 principal components:")
results_pca_8 = grid_search_func(X_train_pca_8, y_train_bal, X_test_pca_8, y_test_bal, models, param_grids)
results_pca_8

"""Additionally, we tested two other models: SVM and Gradient Boosting, with the aim of finding the best model for stroke prediction on our data. As before, we searched for the best hyperparameters and printed the evaluation metrics for each model, both on all features and after dimensionality reduction using PCA with 2 and 8 components"""

# Define the models
new_models = {
        'SVM': SVC(random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(random_state=42)
        }

# Define parameter grids for each model
new_param_grids = {
        'SVM': {
            'C': [0.1, 1, 10],
            'kernel': ['linear', 'rbf', 'poly'],
        },
        'Gradient Boosting': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
    }

# Print the results table
results_df = grid_search_func(X_train_bal, y_train_bal, X_test_bal, y_test_bal, new_models, new_param_grids)
results_df

print("Training models with the first 2 principal components:")
results_pca_2 = grid_search_func(X_train_pca_2, y_train_bal, X_test_pca_2, y_test_bal, new_models, new_param_grids)
results_pca_2

print("Training models with the first 8 principal components:")
results_pca_8 = grid_search_func(X_train_pca_8, y_train_bal, X_test_pca_8, y_test_bal, new_models, new_param_grids)
results_pca_8